{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "similarity_glove.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valeriya-khan/image-captioning/blob/main/similarity_glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bfYs3MKNztG"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LkIDPmoYG6n"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e10RjYPaOGoD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obn8vz_fYZ4k"
      },
      "source": [
        "import os\n",
        "from gensim.parsing.preprocessing import strip_punctuation\n",
        "import gensim.downloader as api\n",
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N6cLIilYbLr"
      },
      "source": [
        "path = \"/content/drive/Team Drives/ROCS (HACKNU)/word2vec/GoogleNews-vectors-negative300.bin\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W5mqee9Ocya"
      },
      "source": [
        "model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uziu4K1ja5Ap"
      },
      "source": [
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CGThflEeY3B"
      },
      "source": [
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izeENOckNGfs"
      },
      "source": [
        "f = open(\"/content/drive/Team Drives/ROCS (HACKNU)/output.txt\",\"w+\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rVBwMk2i583"
      },
      "source": [
        "captions = [line.rstrip('\\n') for line in open('/content/drive/Team Drives/ROCS (HACKNU)/predicted_captions.txt') if line.strip()]\n",
        "inputs = [line.rstrip('\\n') for line in open('/content/drive/Team Drives/ROCS (HACKNU)/inputs.txt') if line.strip()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktCWalmXfqFi"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "  \n",
        "  \n",
        "stop_words = set(stopwords.words('english')) \n",
        "skipped_input = []\n",
        "skipped_caption=[]\n",
        "for i in range(0,25):\n",
        "  input_sent = strip_punctuation(inputs[i]).lower().split()\n",
        "  filtered_inputs = [w for w in input_sent if not w in stop_words]\n",
        "  input_words =[x for x in filtered_inputs if x in word_vectors.vocab]\n",
        "  skipped_input.append([x for x in filtered_inputs if x not in word_vectors.vocab])\n",
        "  similarity=[]\n",
        "  for j in range(0,6):\n",
        "    captions_sent = strip_punctuation(captions[6*i+j]).lower().split()\n",
        "    filtered_captions = [w for w in captions_sent if not w in stop_words]\n",
        "    skipped_caption.append([x for x in filtered_captions if x not in word_vectors.vocab])\n",
        "    cap_words = [x for x in filtered_captions if x in word_vectors.vocab]\n",
        "    sim = word_vectors.n_similarity(cap_words, input_words)\n",
        "    similarity.append(sim)\n",
        "    f.write(\"{:.4f}\".format(sim)+\" \")\n",
        "  k = similarity.index(max(similarity))+1\n",
        "  f.write(\"%i %i\\n\\n\" %(k,(i+1)))\n",
        "f.close()\n",
        "\n",
        "# filtered_captions = [w for w in sentence_obama if not w in stop_words] \n",
        "# filtered_inputs = [w for w in sentence_president if not w in stop_words] \n",
        "\n",
        "# print(sentence_obama) \n",
        "# print(filtered_sentence1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74nz9mlyd7S6"
      },
      "source": [
        "sim = word_vectors.n_similarity(filtered_sentence, filtered_sentence1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPP9hchGeRkR"
      },
      "source": [
        "print(\"{:.4f}\".format(sim))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}