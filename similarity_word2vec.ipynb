{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "similarity_word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valeriya-khan/image-captioning/blob/main/similarity_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bfYs3MKNztG"
      },
      "source": [
        "!pip install gensim\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LkIDPmoYG6n"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obn8vz_fYZ4k"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "from gensim.parsing.preprocessing import strip_punctuation\n",
        "import gensim.downloader as api\n",
        "import gensim\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "skipped_input = []\n",
        "skipped_caption=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxkuHtjA7WAJ"
      },
      "source": [
        "path = \"/content/drive/Team Drives/ROCS (HACKNU)/word2vec/GoogleNews-vectors-negative300.bin\"\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiD3RQm97XcQ"
      },
      "source": [
        "# preprocessing\n",
        "import re\n",
        "import unicodedata\n",
        "import inflect\n",
        "\n",
        "# must-have\n",
        "import scipy\n",
        "import numpy as np\n",
        "\n",
        "# image classification model\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.inception_resnet_v2 import preprocess_input, decode_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdhpoR7p4_gj"
      },
      "source": [
        "# load pretrained spaCy\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLjghbJ45KEw"
      },
      "source": [
        "def remove_non_ascii(words):\n",
        "  \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "  new_words = []\n",
        "  for word in words:\n",
        "    new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    new_words.append(new_word)\n",
        "  return new_words\n",
        "\n",
        "\n",
        "def to_lowercase(words):\n",
        "  \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "  new_words = []\n",
        "  for word in words:\n",
        "    new_word = word.lower()\n",
        "    new_words.append(new_word)\n",
        "  return new_words\n",
        "\n",
        "\n",
        "def remove_punctuation(words):\n",
        "  \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "  new_words = []\n",
        "  for word in words:\n",
        "    new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "    if new_word != '':\n",
        "      new_words.append(new_word)\n",
        "  return new_words\n",
        "\n",
        "\n",
        "def replace_numbers(words):\n",
        "  \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "  p = inflect.engine()\n",
        "  new_words = []\n",
        "  for word in words:\n",
        "    if word.isdigit():\n",
        "      new_word = p.number_to_words(word)\n",
        "      new_words.append(new_word)\n",
        "    else:\n",
        "      new_words.append(word)\n",
        "  return new_words\n",
        "\n",
        "\n",
        "def remove_stopwords(words):\n",
        "  \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "  new_words = []\n",
        "  for word in words:\n",
        "    if word not in nltk.corpus.stopwords.words('english'):\n",
        "      new_words.append(word)\n",
        "  return new_words\n",
        "\n",
        "\n",
        "def stem_words(words):\n",
        "  \"\"\"Stem words in list of tokenized words\"\"\"\n",
        "  stemmer = LancasterStemmer()\n",
        "  stems = []\n",
        "  for word in words:\n",
        "    stem = stemmer.stem(word)\n",
        "    stems.append(stem)\n",
        "  return stems\n",
        "\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "  \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmas = []\n",
        "  for word in words:\n",
        "    lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "    lemmas.append(lemma)\n",
        "  return lemmas\n",
        "\n",
        "\n",
        "def normalize(words):\n",
        "  \"\"\"Normalize words using methods above\"\"\"\n",
        "  words = remove_non_ascii(words)\n",
        "  words = to_lowercase(words)\n",
        "  words = remove_punctuation(words)\n",
        "  words = replace_numbers(words)\n",
        "  words = remove_stopwords(words)\n",
        "  return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0DWJvUw7q-S"
      },
      "source": [
        "clf_model = InceptionResNetV2(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSruBNek5P0i"
      },
      "source": [
        "def get_clf_similarities(test_num, test_path, top=1):\n",
        "  # RENAME THIS AS NEEDED\n",
        "  # test_path = \"/content/drive/Team Drives/ROCS (HACKNU)/test-updated\"\n",
        "  \n",
        "  \"\"\"Calculate similarities between query and top-1 classification result\"\"\"\n",
        "  #print(f\"----- Calculating similarity using image classification -----\")\n",
        "  \n",
        "  # read query\n",
        "  with open(os.path.join(test_path, str(test_num), \"input\"), \"r\") as file:\n",
        "    # read file\n",
        "    raw_query = file.read()\n",
        "    # tokenize\n",
        "    tokens = nltk.word_tokenize(raw_query)\n",
        "    # normalization\n",
        "    query = normalize(tokens)\n",
        "  #  print(f\"Query: {query}\")\n",
        "\n",
        "  # calculate similarities for image classification\n",
        "  similarities = [0.0]*6\n",
        "  \n",
        "  # iterating over all images in test case\n",
        "  for img_path in glob.glob(os.path.join(test_path, str(test_num), \"*.jpg\")):\n",
        "    candidate_img = img_path.split(\"/\")[-1][:-4]\n",
        "    # print(f\"Candidate image: {candidate_img}\")\n",
        "    \n",
        "    # read image from defined path\n",
        "    img = image.load_img(img_path, target_size=(299, 299))\n",
        "    \n",
        "    # convert image to np array and preprocess\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    # predict image label(s)\n",
        "    preds = clf_model.predict(x)\n",
        "    # decode the results into a list of tuples (class, description, probability)\n",
        "    decoded = decode_predictions(preds, top=top)[0]\n",
        "    # print(f\"Predicted: {decoded}\")\n",
        "    \n",
        "    img_sim = 0\n",
        "    # iterating over all words in current query\n",
        "    for qword in query:\n",
        "      # print(f\"\\tQuery word: {qword}\")\n",
        "      pred = decoded[0][1]\n",
        "      # lower and split by \"_\"\n",
        "      pred = pred.lower()\n",
        "      pred = pred.split(\"_\")\n",
        "      # print(f\"\\tClassification prediction(s): {pred}\")\n",
        "      \n",
        "      for pword in pred:\n",
        "        sim = nlp(qword).similarity(nlp(pword))\n",
        "        # add to candidate similarity\n",
        "        img_sim += sim\n",
        "        # print(f\"\\t\\tSimilarity of '{qword}' with '{pword}': {sim}\")\n",
        "    qr = 0\n",
        "    for u in query:\n",
        "      qr = qr + 1\n",
        "    pr = 0\n",
        "    for v in pred:\n",
        "      pr = pr + 1\n",
        "    #print(pred)\n",
        "    #print(query)\n",
        "    #qr = len(query)\n",
        "    #print(qr)\n",
        "    \n",
        "    #pr = len(pred)\n",
        "    img_sim = img_sim/pr\n",
        "    img_sim = img_sim/qr\n",
        "    # print(f\"\\tCandidate prediction similarity (after dividing by {len(pred)*len(query)}): {img_sim}\")\n",
        "    # print(\"\\t------------------------------\")\n",
        "    similarities[int(candidate_img)-1] = img_sim\n",
        "    \n",
        " # print(f\"Similarities for query: {similarities}\")\n",
        "  best = np.argmax(similarities)+1\n",
        "  #print(f\"Best match: {best}\")\n",
        "  #print(f\"--------------------------------------------------\")\n",
        "  return best"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi2k4a1RYwy1"
      },
      "source": [
        "## Edit this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rVBwMk2i583"
      },
      "source": [
        "captions = [line.rstrip('\\n') for line in open('/content/drive/Team Drives/ROCS (HACKNU)/predicted_captions.txt') if line.strip()]\n",
        "inputs = [line.rstrip('\\n') for line in open('/content/drive/Team Drives/ROCS (HACKNU)/inputs.txt') if line.strip()]\n",
        "captions1 = [line.rstrip('\\n') for line in open('/content/drive/Team Drives/ROCS (HACKNU)/predicted_captions_2nd-ver.txt') if line.strip()]\n",
        "testdir='drive/Team Drives/ROCS (HACKNU)/Archive2/'\n",
        "outfile = open('drive/Team Drives/ROCS (HACKNU)/rocs_outfile.txt', \"w\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L3sC_txqeZ8"
      },
      "source": [
        "def writeOutput(test_dir,i,numb):\n",
        "    with open(test_dir  + str(i)+'/output', 'w') as f:\n",
        "           f.write(\"%i\" % numb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3FQktV-ONrG"
      },
      "source": [
        "def prob_calc(i,captions):\n",
        "  similarity=[]\n",
        "  for j in range(0,6):\n",
        "    captions_sent = strip_punctuation(captions[6*i+j]).lower().split()\n",
        "    filtered_captions = [w for w in captions_sent if not w in stop_words]\n",
        "    skipped_caption.append([x for x in filtered_captions if x not in model.vocab])\n",
        "    cap_words = [x for x in filtered_captions if x in model.vocab]\n",
        "    sim = model.n_similarity(cap_words, input_words)\n",
        "    similarity.append(sim)\n",
        "  prob=sorted(similarity[:],reverse=True)[0]-sorted(similarity[:],reverse=True)[1]\n",
        "  return prob, similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktCWalmXfqFi"
      },
      "source": [
        "MAX_DIR=70\n",
        "listik= []\n",
        "for i in range(0,MAX_DIR):\n",
        "  input_sent = strip_punctuation(inputs[i]).lower().split()\n",
        "  filtered_inputs = [w for w in input_sent if not w in stop_words]\n",
        "  input_words =[x for x in filtered_inputs if x in model.vocab]\n",
        "  skipped_input.append([x for x in filtered_inputs if x not in model.vocab])\n",
        "  similarity=[]\n",
        "#   for j in range(0,6):\n",
        "#     captions_sent = strip_punctuation(captions[6*i+j]).lower().split()\n",
        "#     filtered_captions = [w for w in captions_sent if not w in stop_words]\n",
        "#     skipped_caption.append([x for x in filtered_captions if x not in model.vocab])\n",
        "#     cap_words = [x for x in filtered_captions if x in model.vocab]\n",
        "#     sim = model.n_similarity(cap_words, input_words)\n",
        "#     similarity.append(sim)\n",
        "    \n",
        "#   k = similarity.index(max(similarity))+1\n",
        "#   f.write(\"%i %i\\n\\n\" %(k,(i+1)))\n",
        "#   prob = sorted(similarity[:],reverse=True)[0]-sorted(similarity[:],reverse=True)[1]\n",
        "  prob,similarity=prob_calc(i,captions)\n",
        "  \n",
        "  if prob>0.06:\n",
        "#     [f.write(\"{:.4f}\".format(word)+\" \") for word in similarity]\n",
        "#     f.write(\"/n\")\n",
        "    k=similarity.index(max(similarity))+1\n",
        "    listik.append(k)\n",
        "    #outfile.write(str(k)+\"\\n\")\n",
        "    print(similarity.index(max(similarity))+1,i+1)\n",
        "    writeOutput(testdir,i+1,similarity.index(max(similarity))+1)\n",
        "  else:\n",
        "    similarity=[]\n",
        "#     for j in range(0,6):\n",
        "#       captions_sent = strip_punctuation(captions1[6*i+j]).lower().split()\n",
        "#       filtered_captions = [w for w in captions_sent if not w in stop_words]\n",
        "#       skipped_caption.append([x for x in filtered_captions if x not in model.vocab])\n",
        "#       cap_words = [x for x in filtered_captions if x in model.vocab]\n",
        "#       sim = model.n_similarity(cap_words, input_words)\n",
        "#       similarity.append(sim)\n",
        "#     prob = sorted(similarity[:],reverse=True)[0]-sorted(similarity[:],reverse=True)[1]\n",
        "    prob,similarity=prob_calc(i,captions1)\n",
        "    if prob>0.06:\n",
        "#       [f.write(\"{:.4f}\".format(word)+\" \") for word in similarity]\n",
        "#       f.write(\"/n\")\n",
        "      k=similarity.index(max(similarity))+1\n",
        "      listik.append(k)\n",
        "      #outfile.write(str(k)+\"\\n\")\n",
        "      print(similarity.index(max(similarity))+1,i+1)\n",
        "      writeOutput(testdir,i+1,similarity.index(max(similarity))+1)\n",
        "    else:\n",
        "      ind=get_clf_similarities(i+1,testdir)\n",
        "      writeOutput(testdir,i+1,ind)\n",
        "      #outfile.write(str(ind)+\"\\n\")\n",
        "      listik.append(ind)\n",
        "      print(ind,i+1)\n",
        "# f.close()\n",
        "\n",
        "\n",
        "# filtered_captions = [w for w in sentence_obama if not w in stop_words] \n",
        "# filtered_inputs = [w for w in sentence_president if not w in stop_words] \n",
        "\n",
        "# print(sentence_obama) \n",
        "# print(filtered_sentence1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98LqfGixcujL"
      },
      "source": [
        "lenik= 0\n",
        "\n",
        "for up in listik:\n",
        "  lenik = lenik + 1\n",
        "\n",
        "print(f\"len: {lenik}\")\n",
        "print(f\"list: {listik}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrafP4Mtce1b"
      },
      "source": [
        "with open(\"drive/Team Drives/ROCS (HACKNU)/rocs_outfile.txt\", \"w\") as outfile:\n",
        "  for l in listik:\n",
        "    outfile.write(str(l)+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX3SrZVBQiSf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}